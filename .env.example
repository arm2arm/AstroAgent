# LLM Endpoint Profiles (OpenAI-compatible /v1)
# Add as many profiles as needed: LLM_1_*, LLM_2_*, LLM_3_*, ...
# The first profile is selected by default.

LLM_1_NAME=Ollama Local
LLM_1_BASE_URL=http://localhost:11434/v1
LLM_1_API_KEY=
LLM_1_MODEL=qwen3-coder:latest
LLM_1_CONTEXT_WINDOW=32768
LLM_1_OUTPUT_BUDGET=8192
LLM_1_EMBED_MODEL=nomic-embed-text:latest
LLM_1_EMBED_PROVIDER=ollama
LLM_1_EMBED_BASE_URL=
LLM_1_EMBED_API_KEY=

LLM_2_NAME=AIP
LLM_2_BASE_URL=https://ai.aip.de/api
LLM_2_API_KEY=your-api-key-here
LLM_2_MODEL=Qwen2.5-32B-Instruct
LLM_2_CONTEXT_WINDOW=32768
LLM_2_OUTPUT_BUDGET=8192
LLM_2_EMBED_MODEL=
LLM_2_EMBED_PROVIDER=openai
LLM_2_EMBED_BASE_URL=
LLM_2_EMBED_API_KEY=

# LLM Parameters
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=4000
LLM_TIMEOUT=120
LLM_CONTEXT_WINDOW=32768
LLM_OUTPUT_BUDGET=8192
LLM_SAFETY_MARGIN=512
LLM_SUMMARY_TRIGGER_TOKENS=2000
LLM_SUMMARY_TARGET_TOKENS=600

# Embeddings (optional separate endpoint)
EMBED_MODEL=nomic-embed-text:latest
EMBED_PROVIDER=ollama
EMBED_BASE_URL=
EMBED_API_KEY=

# Code Execution Environment (Docker sandbox)
# EXEC_MODE: "image" = pull remote image, "dockerfile" = build from local Dockerfile
EXEC_MODE=image
EXEC_IMAGE=astroagent-exec:latest
EXEC_DOCKERFILE=./Dockerfile.executor
EXEC_PRE_INSTALL=numpy,pandas,matplotlib,astropy,scipy

# Workflow Configuration
OUTPUT_DIR=outputs/workflows
RESULTS_DIR=outputs/results
MAX_RETRIES=3
MAX_REVIEW_ITERATIONS=3
VERBOSE=true

# Memory / RAG Configuration (SQLite + FTS5)
MEMORY_ENABLED=true
MEMORY_DB_PATH=.crewai/memory_astroagent.db
MEMORY_INDEX_PATHS=README.md,QUICKSTART.md,project.md,example_tasks
MEMORY_CHUNK_TOKENS=400
MEMORY_TOP_K=4
MEMORY_FORCE_REINDEX=false
