# ============================================================================
# AstroAgent Configuration (.env)
# ============================================================================
# Single LLM endpoint — all agents share this configuration.
# Agent prompts are in config/agents.yaml, task templates in config/tasks.yaml.

# LLM Endpoint (OpenAI-compatible /v1)
LLM_BASE_URL=http://localhost:11434/v1
LLM_API_KEY=
LLM_MODEL=qwen3-coder:latest

# CrewAI native provider (openai, anthropic, azure, gemini, bedrock)
# Use "openai" for any OpenAI-compatible endpoint (Ollama, vLLM, LM Studio, etc.)
LLM_PROVIDER=openai

# LLM Parameters
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=4000
LLM_TIMEOUT=120
LLM_CONTEXT_WINDOW=32768
LLM_OUTPUT_BUDGET=8192
LLM_SAFETY_MARGIN=512
LLM_SUMMARY_TRIGGER_TOKENS=2000
LLM_SUMMARY_TARGET_TOKENS=600

# Optional: override model for the Executor agent
EXECUTOR_LLM_MODEL=

# Embeddings (for CrewAI memory — optional)
EMBED_MODEL=nomic-embed-text:latest
EMBED_PROVIDER=ollama
EMBED_BASE_URL=
EMBED_API_KEY=

# Code Execution Environment (Docker sandbox)
# EXEC_MODE: "image" = pull remote image, "dockerfile" = build from local Dockerfile
EXEC_MODE=image
EXEC_IMAGE=astroagent-exec:latest
EXEC_DOCKERFILE=./Dockerfile.executor
EXEC_PRE_INSTALL=numpy,pandas,matplotlib,astropy,scipy

# Workflow Configuration
OUTPUT_DIR=outputs/workflows
RESULTS_DIR=outputs/results
MAX_RETRIES=3
MAX_REVIEW_ITERATIONS=3
VERBOSE=true

# Memory / Storage (CrewAI native SQLite long-term memory)
MEMORY_ENABLED=true
MEMORY_DB_PATH=storage/memory.db
